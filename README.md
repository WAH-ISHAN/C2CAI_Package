  

---

# ğŸ“˜ C2CAI â€“ Voice Assistant Widget for Portfolio / Websites  

**C2CAI** ğŸ§ ğŸ™ï¸ is a **frontend voice widget** built using the Web Speech API (SpeechRecognition + SpeechSynthesis).  
This allows you to talk to your portfolio/site and get spoken answers back.  

- âœ… Works out of the box on **Chrome / Edge** (with mic permissions)  
- ğŸŒ Sinhala + English language support (default Sinhala welcome: *à¶†à¶ºà·”à¶¶à·à·€à¶±à·Š! à¶¸à¶§ à¶šà¶­à· à¶šà¶»à¶±à·Šà¶±.*)  
- ğŸ”‰ Converts your speech â†’ text â†’ generates response â†’ speaks back  
- ğŸ“¦ Small, pluggable widget: Just drop into your React/Vite/Next or plain HTML site  

---

## ğŸš€ Features
- Speech Recognition using Web APIs  
- Textâ€‘toâ€‘Speech feedback  
- Sinhala/English support (auto detects userLang)  
- Simple `<div>` integration (like a chatbot button)  
- Can run in **two modes**:
  1. **Echoâ€‘only (no backend)** â†’ repeats back your words (for demos & portfolios)  
  2. **AIâ€‘powered (with backend)** â†’ connect to Node.js `/chat` API (LLM/RAG etc)  

---

## ğŸ“‚ Project Structure
```
C2CAI_Package/
 â”œâ”€â”€ src/
 â”‚    â”œâ”€â”€ client/widget.ts   # Browser widget
 â”‚    â”œâ”€â”€ server.ts          # Node/Express backend (optional)
 â”‚    â”œâ”€â”€ rag.ts             # RAG logic (optional advanced)
 â”‚    â””â”€â”€ ...                
 â””â”€â”€ dist/                   # Build outputs (CJS/ESM/Widget bundle)
```

---

## ğŸ”§ Installation & Build
```bash
# setup deps
npm install   

# build both client/server
npm run build
```

Build step generates:
- `dist/client/widget.js` â†’ the widget you embed in your site  
- `dist/server.js` â†’ backend server with `/chat` endpoint  

---

## ğŸ–‡ï¸ How to Use C2CAI in a Portfolio  

### 1. Copy widget to your portfolio project
Take the `dist/client/widget.js` â†’ copy into your Portfolio repoâ€™s `public/` folder:

```
/portfolio-site
 â””â”€â”€ public/
     â””â”€â”€ c2cai-widget.js
```

---

### 2. Add container + init script

In **React/Vite App** (`App.jsx`):

```jsx
import { useEffect } from 'react';

function App() {
  useEffect(() => {
    const s = document.createElement("script");
    s.src = "/c2cai-widget.js"; // public path
    s.onload = () => {
      window.initC2CAI?.({
        containerId: "c2cai-widget",
        // if backend: serverUrl: "https://your-backend.com"
      });
    };
    document.body.appendChild(s);

    return () => window.speechSynthesis?.cancel?.();
  }, []);

  return (
    <div>
      <h1>My Portfolio</h1>
      {/* assistant mounts here */}
      <div id="c2cai-widget"></div>
    </div>
  )
}
export default App;
```

---

### 3. Without a backend (Echo mode)
- If you donâ€™t want to set up a server â†’ widget can just **echo** your words back:  
  - In `handleQuery(query)` â†’ remove fetch, instead do:  
    ```ts
    const response = `You said: ${query}`;
    this.speak(response, this.recognition?.lang || "en-US");
    ```
- Result: voice assistant repeats what you say. This is nice for a **Portfolio demo** with ZERO backend hosting. âœ…

---

### 4. With backend (AIâ€‘mode)
/chat endpoint â†’ Node server (e.g. Express):

```ts
import express from "express";
import cors from "cors";

const app = express();
app.use(cors());
app.use(express.json());

app.post("/chat", async (req, res) => {
  const { query } = req.body;
  if (!query) return res.status(400).json({ response: "Missing query" });

  // TODO: connect LLM or RAG here
  res.json({ response: "Echo: " + query });
});

app.listen(3000, () => console.log("C2CAI server on 3000"));
```

â¡ï¸ Deploy this server to **Render / Railway / Fly.io** and pass its URL into your widget init `serverUrl`.

---

### 5. Supported frameworks
- Plain HTML:
  ```html
  <div id="c2cai-widget"></div>
  <script src="/c2cai-widget.js"></script>
  <script>
    window.initC2CAI({
      serverUrl: "http://localhost:3000",
      containerId: "c2cai-widget"
    });
  </script>
  ```
- React (Vite, CRA) â†’ useEffect as shown above.  
- Next.js (Vercel) â†’ use `<div id="c2cai-widget"></div>` + load script from `public/`.

---

## ğŸ§ª Demo Flow
1. Click **Start Voice Assistant** button.  
2. Browser asks mic permission.  
3. Speak â†’ widget captures speech.  
4. If **mock mode**: Assistant repeats: â€œYou said: â€¦â€.  
5. If **backend mode**: Widget sends query to `/chat`, gets AI response, speaks it out.  

---

## ğŸ¨ Styling
Button can be customized in `App.css`:
```css
#c2cai-widget button {
  background: #111;
  color: #fff;
  border-radius: 6px;
  padding: 10px 16px;
  font-weight: bold;
  cursor: pointer;
}
#c2cai-widget button:hover {
  background: #333;
}
```

---

## ğŸŒ Browser Support
- Chrome / Edge âœ…  
- Safari (macOS/iOS) âš ï¸ (`webkitSpeechRecognition` only)  
- Firefox âŒ (no native recognition yet)

---

## ğŸ›¡ï¸ Notes
- **HTTPS required** in production (to use mic)  
- **Backend needed** only if real AI answers are required  
- Without backend â†’ safe echo demo (ideal for portfolio, no secrets exposed)  
- With backend â†’ hide API keys in server only, never expose in frontend  

---

## ğŸ—ï¸ Roadmap
- Streaming responses  
- Multiâ€‘language voice selection  
- Plugâ€‘andâ€‘play OpenAI/Anthropic adapters  
- RAG support (document crawl & retrieval)  

---

## ğŸ“œ License
MIT â€“ Free to use in your portfolio or projects.

---

ğŸ‘‰ **Summary:**  
Add `c2cai-widget.js` in your **public/**, mount `<div id="c2cai-widget">`, init it.  
- No backend? â†’ it echoes back what you say (fun interactive demo).  
- With backend? â†’ it becomes a true **AI assistant** fetching answers from `/chat`.  

---
## License
This project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.
----
